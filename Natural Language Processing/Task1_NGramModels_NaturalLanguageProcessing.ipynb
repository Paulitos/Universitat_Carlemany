{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - NGram Models"
      ],
      "metadata": {
        "id": "XKeWkSk36fWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Write a python program that **takes a dataset and generates n-grams** to a given value of n. Use the generated n-grams to conduct a **frequency analysis**. What is the most common n-grams in the provided text? Relate the results to the theory behind n-grams and their importance in language representation."
      ],
      "metadata": {
        "id": "sFxvkMku6v2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q-dcfXu6YDC",
        "outputId": "766284fa-e9b0-4494-87db-0c24d9e7acd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams:\n",
            "pic twitter com: 3511\n",
            "https t co: 2085\n",
            "red dead redemption: 1077\n",
            "i can t: 937\n",
            "_ _ _: 936\n",
            "call of duty: 835\n",
            "italy italy italy: 761\n",
            "i don t: 735\n",
            "dead redemption 2: 708\n",
            "league of legends: 651\n"
          ]
        }
      ],
      "source": [
        "# I will use this dataset: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Function to read the dataset\n",
        "def read_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Function to generate n-grams\n",
        "def generate_ngrams(text, n):\n",
        "    # Clean and tokenize text\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    tokens = text.lower().split()\n",
        "\n",
        "    # Generate n-grams\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    ngrams = [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "# Function for n-gram frequency analysis\n",
        "def ngram_frequency_analysis(text, n):\n",
        "    ngrams = generate_ngrams(text, n)\n",
        "    ngram_freq = Counter(ngrams)\n",
        "\n",
        "    return ngram_freq\n",
        "\n",
        "# Read csv file\n",
        "file_path = '/content/twitter_training_clean.csv'  # Replace with the path to your file\n",
        "df = read_csv(file_path)\n",
        "\n",
        "# Select the text column you want to analyze\n",
        "column_to_analyze = 'Tweet content'  # Make sure that the column exists in your dataset\n",
        "text_data = ' '.join(df[column_to_analyze].dropna().astype(str).tolist())\n",
        "\n",
        "n = 3  # Change this value to generate different n-grams (e.g., 1 for unigrams, 2 for bigrams, etc.)\n",
        "ngram_freq = ngram_frequency_analysis(text_data, n)\n",
        "\n",
        "# Find the most common n-grams\n",
        "most_common_ngrams = ngram_freq.most_common(10)  # Top 10 most common n-grams\n",
        "\n",
        "print(\"Most common n-grams:\")\n",
        "for ngram, freq in most_common_ngrams:\n",
        "    print(f\"{ngram}: {freq}\")\n",
        "\n",
        "# Relating the results to the theory behind n-grams\n",
        "# N-grams are a fundamental concept in text mining and natural language processing. They represent\n",
        "# sequences of words or tokens in a given text and are used to capture the context and relationship\n",
        "# between words. For instance, unigrams (n=1) give the frequency of individual words, bigrams (n=2)\n",
        "# give the frequency of word pairs, and trigrams (n=3) give the frequency of word triplets.\n",
        "\n",
        "# The frequency analysis of n-grams is crucial for understanding language patterns, identifying common phrases,\n",
        "# and building language models. In tasks such as text prediction, speech recognition, and machine translation,\n",
        "# n-grams help in modeling the likelihood of a word given the previous words, thus enabling more accurate predictions\n",
        "# and translations.\n",
        "\n",
        "# In the example text provided, the most common trigrams (n=3) help identify prevalent phrases and contextual word\n",
        "# associations within the text. This information can be used to improve algorithms in natural language processing\n",
        "# applications."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "**N-grams Theory and Importance**\n",
        "\n",
        "1. **Definition:**\n",
        "\n",
        "*   N-grams are contiguous sequences of 'n' items (words, characters, etc.) from a given text.\n",
        "*   For example, in the sentence \"I love NLP\", unigrams are \\[\"I\", \"love\", \"NLP\"\\], bigrams are \\[\"I love\", \"love NLP\"\\], and trigrams are \\[\"I love NLP\"\\].\n",
        "\n",
        "2. **Importance in Language Represention:**\n",
        "\n",
        "* **Context Capture:** N-grams capture the local context of words. Higher-order n-grams (bigrams, trigrams) provide more context than unigrams.\n",
        "* **Predictive Modeling:** In language models, n-grams are used to predict the next word given the previous 'n-1' words, improving the accuracy of predictions.\n",
        "* **Text Analysis:** Frequency analysis of n-grams helps identify common phrases, understand text structure, and extract meaningful patterns.\n",
        "* **Applications:** N-grams are used in various NLP tasks like text generation, machine translation, sentiment analysis, and more.\n",
        "\n",
        "By analyzing the most common n-grams in a text, we can gain insights into the text's structure, common phrases, and thematic elements. This information is crucial for building effective natural language processing systems.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GIF8SzOk7rv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of the Most Common N-grams\n",
        "\n",
        "1. **pic twitter com: 3511**\n",
        "   - This is a common combination in tweets containing links to Twitter images. The prefix “pic” usually precedes “twitter.com” in shared image links.\n",
        "\n",
        "2. **https t co: 2085**.\n",
        "   - Similar to the above, this n-gram is common in tweets that include links shortened using Twitter's URL shortening service, `t.co`.\n",
        "\n",
        "3. **red dead redemption: 1077**\n",
        "   - This is a name of a popular game, indicating that the game `red dead redemption` was a common theme in the tweets analyzed.\n",
        "\n",
        "4. **i can t: 937**\n",
        "   - A common n-gram in everyday language, showing a phrase commonly used in conversation.\n",
        "\n",
        "5. **_ _ _: 936**\n",
        "   - This could be an artifact of some formatting in the tweets (e.g., underscore characters used for emphasis or separators). It should be investigated further.\n",
        "\n",
        "6. **call of duty: 835**\n",
        "   - Another name of a popular game, indicating that “Call of Duty” was also a frequent theme in tweets.\n",
        "\n",
        "7. **italy italy italy: 761**\n",
        "   - This could be a repeating pattern used in some tweets, perhaps related to events in Italy or Italy-related spam.\n",
        "\n",
        "8. **i don t: 735**\n",
        "   - Another common phrase in everyday language, reflecting how people speak and express negations.\n",
        "\n",
        "9. **dead redemption 2: 708**\n",
        "   - Part of the name of the game “Red Dead Redemption 2”, reinforcing that this game was a much-discussed topic.\n",
        "\n",
        "10. **league of legends: 651**\n",
        "    - The name of another popular game, “League of Legends,” which was also a recurring theme in tweets.\n",
        "\n"
      ],
      "metadata": {
        "id": "w3PObjuyPLos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implement a **simple language model** using n-grams. Use the unigrams, bigrams and / or trigrams that you have found to compute the MLE probability of a word given some context. Apply it for for at least **four different pairs of word and context**. Apply some **smoothing** and justify why you are doing it. Play with **different smoothing K's** and explain the differences between the methods you try. Do all methods agree about which n-gram has higher probability? Comment on your results."
      ],
      "metadata": {
        "id": "oDstFe1vP4UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the text column you want to analyze\n",
        "column_to_analyze = 'Tweet content'  # Make sure that the column exists in your dataset\n",
        "text_data = ' '.join(df[column_to_analyze].dropna().astype(str).tolist())\n",
        "\n",
        "# Generate n-grams for smoothing\n",
        "unigrams = generate_ngrams(text_data, 1)\n",
        "bigrams = generate_ngrams(text_data, 2)\n",
        "trigrams = generate_ngrams(text_data, 3)\n",
        "\n",
        "# Calculate the frequencies of the n-grams for smoothing\n",
        "unigram_freq = Counter(unigrams)\n",
        "bigram_freq = Counter(bigrams)\n",
        "trigram_freq = Counter(trigrams)\n",
        "\n",
        "# Calculate the frequencies of the n-gram contexts for smoothing\n",
        "bigram_context_freq = Counter([bigram.split(' ')[0] for bigram in bigrams])\n",
        "trigram_context_freq = Counter([tuple(trigram.split(' ')[0:2]) for trigram in trigrams])\n",
        "\n",
        "# Vocabulary size for smoothing\n",
        "vocab_size = len(set(unigrams))\n",
        "\n",
        "# Select word-context pairs based on frequent n-grams\n",
        "def select_word_context_pairs(frequent_ngrams, context_size):\n",
        "    word_context_pairs = []\n",
        "    for ngram, freq in frequent_ngrams.most_common(4):\n",
        "        words = ngram.split()\n",
        "        word = words[-1]\n",
        "        context = words[:-1]\n",
        "        word_context_pairs.append((word, tuple(context)))  # Convert context to tuple\n",
        "    return word_context_pairs\n",
        "\n",
        "# Get the most common bigrams and trigrams\n",
        "most_common_bigrams = bigram_freq.most_common(4)\n",
        "most_common_trigrams = trigram_freq.most_common(4)\n",
        "\n",
        "# Combine the most common bigrams and trigrams to form word-context pairs\n",
        "word_context_pairs = select_word_context_pairs(bigram_freq, 1) + select_word_context_pairs(trigram_freq, 2)\n",
        "\n",
        "# Ensure unique pairs\n",
        "word_context_pairs = list(set(word_context_pairs))\n",
        "\n",
        "# Function for calculating the maximum likelihood estimate (MLE) with smoothing\n",
        "def calculate_mle_with_smoothing(word, context, ngram_freq, ngram_context_freq, vocab_size, k=1):\n",
        "    context_str = ' '.join(context)\n",
        "    context_freq = ngram_context_freq.get(context_str, 0)\n",
        "    word_given_context_freq = ngram_freq.get(' '.join(context + (word,)), 0)  # Concatenate context with word as tuple\n",
        "\n",
        "    # Apply Laplace smoothing\n",
        "    mle_prob = (word_given_context_freq + k) / (context_freq + k * vocab_size)\n",
        "\n",
        "    return mle_prob\n",
        "\n",
        "# Apply smoothing and calculate the maximum likelihood probability for each word pair and context\n",
        "for word, context in word_context_pairs:\n",
        "    context_str = ' '.join(context)\n",
        "    print(f\"Word: '{word}', Context: '{context_str}'\")\n",
        "\n",
        "    print(\"Unigram MLE probabilities:\")\n",
        "    for k in [0.1, 0.5, 1, 5]:\n",
        "        unigram_mle_prob = calculate_mle_with_smoothing(word, context, unigram_freq, unigram_freq, vocab_size, k)\n",
        "        print(f\"K={k}: {unigram_mle_prob:.6f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Bigram MLE probabilities:\")\n",
        "    for k in [0.1, 0.5, 1, 5]:\n",
        "        bigram_mle_prob = calculate_mle_with_smoothing(word, context[:1], bigram_freq, bigram_context_freq, vocab_size, k)\n",
        "        print(f\"K={k}: {bigram_mle_prob:.6f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Trigram MLE probabilities:\")\n",
        "    for k in [0.1, 0.5, 1, 5]:\n",
        "        trigram_mle_prob = calculate_mle_with_smoothing(word, context[:2], trigram_freq, trigram_context_freq, vocab_size, k)\n",
        "        print(f\"K={k}: {trigram_mle_prob:.6f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha1udQRpSt6D",
        "outputId": "85924a17-19d8-4300-f474-1f9c534410ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 't', Context: 'i can'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.000461\n",
            "K=0.5: 0.000358\n",
            "K=1: 0.000282\n",
            "K=5: 0.000120\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 0.301173\n",
            "K=0.5: 0.060260\n",
            "K=1: 0.030146\n",
            "K=5: 0.006055\n",
            "\n",
            "Word: 'twitter', Context: 'pic'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000014\n",
            "K=0.5: 0.000025\n",
            "K=1: 0.000028\n",
            "K=5: 0.000031\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.476760\n",
            "K=0.5: 0.177254\n",
            "K=1: 0.099299\n",
            "K=5: 0.021999\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Word: 'com', Context: 'twitter'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000014\n",
            "K=0.5: 0.000025\n",
            "K=1: 0.000028\n",
            "K=5: 0.000031\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.505432\n",
            "K=0.5: 0.187464\n",
            "K=1: 0.104952\n",
            "K=5: 0.023235\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Word: 'm', Context: 'i'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000003\n",
            "K=0.5: 0.000010\n",
            "K=1: 0.000015\n",
            "K=5: 0.000026\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.115367\n",
            "K=0.5: 0.087613\n",
            "K=1: 0.067361\n",
            "K=5: 0.023657\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Word: 's', Context: 'it'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000005\n",
            "K=0.5: 0.000015\n",
            "K=1: 0.000020\n",
            "K=5: 0.000029\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.176516\n",
            "K=0.5: 0.110945\n",
            "K=1: 0.075771\n",
            "K=5: 0.021445\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Word: 'com', Context: 'pic twitter'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.001371\n",
            "K=0.5: 0.000530\n",
            "K=1: 0.000311\n",
            "K=5: 0.000094\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 1.128427\n",
            "K=0.5: 0.225711\n",
            "K=1: 0.112872\n",
            "K=5: 0.022600\n",
            "\n",
            "Word: 'co', Context: 'https t'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.009413\n",
            "K=0.5: 0.002881\n",
            "K=1: 0.001555\n",
            "K=5: 0.000355\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 0.670127\n",
            "K=0.5: 0.134051\n",
            "K=1: 0.067042\n",
            "K=5: 0.013434\n",
            "\n",
            "Word: 'redemption', Context: 'red dead'\n",
            "Unigram MLE probabilities:\n",
            "K=0.1: 0.000032\n",
            "K=0.5: 0.000032\n",
            "K=1: 0.000032\n",
            "K=5: 0.000032\n",
            "\n",
            "Bigram MLE probabilities:\n",
            "K=0.1: 0.001231\n",
            "K=0.5: 0.000374\n",
            "K=1: 0.000212\n",
            "K=5: 0.000070\n",
            "\n",
            "Trigram MLE probabilities:\n",
            "K=0.1: 0.346167\n",
            "K=0.5: 0.069259\n",
            "K=1: 0.034646\n",
            "K=5: 0.006955\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smoothing is essential in language modeling to handle the issue of zero probabilities for unseen n-grams. Without smoothing, any n-gram not present in the training data would have a probability of zero, which would drastically affect the performance of the model.\n",
        "\n",
        "We applied Laplace smoothing (additive smoothing) with different K-values to observe how the probabilities change. The values tested were K=0.1, 0.5, 1, and 5.\n",
        "\n",
        "\n",
        "#### Observations and Analysis\n",
        "\n",
        "1. **Effect of Smoothing on Unigram Probabilities:**\n",
        "   - The unigram probabilities are very small and do not vary significantly with different K-values. This is because unigrams have a larger context (the entire vocabulary), making individual word probabilities small.\n",
        "\n",
        "2. **Effect of Smoothing on Bigram Probabilities:**\n",
        "   - The bigram probabilities are higher than unigram probabilities, as they consider the preceding word, which reduces the context size.\n",
        "   - Higher K-values tend to reduce the bigram probabilities because the additional counts from smoothing distribute the probability mass more evenly across the vocabulary.\n",
        "\n",
        "3. **Effect of Smoothing on Trigram Probabilities:**\n",
        "   - Trigram probabilities are generally higher than both unigram and bigram probabilities when the context is well-defined and frequent.\n",
        "   - For some word-context pairs, smoothing with higher K-values drastically reduces the probability. This indicates that the trigram model heavily relies on the context, and adding more smoothing dilutes this effect.\n",
        "\n",
        "#### Agreement Among Models\n",
        "\n",
        "The different n-gram models do not always agree on which n-gram has the higher probability. For example:\n",
        "- For the word 't' in the context 'i can', the trigram model assigns a high probability with lower K-values, while the bigram and unigram models assign much lower probabilities.\n",
        "- For the word 'com' in the context 'pic twitter', the trigram model assigns a very high probability with lower K-values, indicating strong context dependence.\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "The n-gram models (unigram, bigram, trigram) show varying probabilities based on the context and the amount of smoothing applied. The trigram model tends to provide higher probabilities for well-defined contexts but is more sensitive to the choice of K-value for smoothing. The unigram model is the least sensitive to context, while the bigram model provides a middle ground.\n",
        "\n",
        "Smoothing is crucial for handling unseen n-grams, and different K-values help balance the trade-off between assigning too much probability mass to seen n-grams and distributing it too evenly across the entire vocabulary. This balance is essential for building robust language models."
      ],
      "metadata": {
        "id": "WpTCsOk7NgTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Use the frequency of n-grams to **predict the next Word in a sentence.** Try your code on a use case of your choice. Explain how you do it, justify the choice of the language model and discuss its limitations."
      ],
      "metadata": {
        "id": "z7Lr1gbBN_Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the next word\n",
        "def predict_next_word(context, ngram_freq, ngram_size):\n",
        "    context_str = ' '.join(context[-(ngram_size-1):])  # Get the relevant part of the context\n",
        "    candidates = {ngram.split()[-1]: freq for ngram, freq in ngram_freq.items() if ngram.startswith(context_str)}\n",
        "\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    # Return the word with the highest frequency\n",
        "    return max(candidates, key=candidates.get)\n",
        "\n",
        "# Use case: Predict the next word for a given context\n",
        "context = ['red', 'dead']  # Example context\n",
        "next_word_bigram = predict_next_word(context, bigram_freq, 2)\n",
        "next_word_trigram = predict_next_word(context, trigram_freq, 3)\n",
        "\n",
        "print(f\"Given context: {' '.join(context)}\")\n",
        "print(f\"Next word prediction using bigram model: {next_word_bigram}\")\n",
        "print(f\"Next word prediction using trigram model: {next_word_trigram}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV4nE3_xP4ZZ",
        "outputId": "26d4e358-d4dd-49e7-d5e9-5c991dcc4404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given context: red dead\n",
            "Next word prediction using bigram model: redemption\n",
            "Next word prediction using trigram model: redemption\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict the next word in a sentence using n-gram frequencies, we can employ a simple language model based on the n-grams we've generated. Here, I'll demonstrate how to use bigram and trigram models for this task. We'll start by calculating the conditional probabilities of potential next words given the current context (last one or two words).\n",
        "\n",
        "**Predicting the Next Word:**\n",
        "   - The `predict_next_word` function takes the context (last word for bigrams, last two words for trigrams), searches the n-grams that match the context, and selects the next word with the highest frequency.\n",
        "\n",
        "### Justification for Choice of Language Model\n",
        "\n",
        "- **Bigram Model:**\n",
        "  - The bigram model considers only the last word of the context. It is computationally less expensive and works well for cases where the immediate predecessor word is highly indicative of the next word.\n",
        "\n",
        "- **Trigram Model:**\n",
        "  - The trigram model takes into account the last two words, capturing more context and providing better predictions for cases where the next word depends on a two-word phrase.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "1. **Data Sparsity:**\n",
        "   - Trigram models suffer from data sparsity, as the probability estimates for trigrams with low counts can be unreliable. This is where smoothing techniques help, but even with smoothing, rare trigrams can skew predictions.\n",
        "\n",
        "2. **Fixed Context Size:**\n",
        "   - Both bigram and trigram models have a fixed context size, limiting their ability to capture longer-range dependencies in language.\n",
        "\n",
        "3. **Vocabulary Size:**\n",
        "   - Large vocabulary sizes can lead to many unseen n-grams in the training data, again necessitating smoothing.\n",
        "\n",
        "4. **No Semantic Understanding:**\n",
        "   - N-gram models do not capture the semantic meaning of words, which can lead to grammatically correct but semantically nonsensical predictions.\n",
        "\n",
        "### Use Case and Predictions\n",
        "\n",
        "In the given example, for the context `['red', 'dead']`, the bigram model might predict 'redemption' (if it follows 'dead' frequently), while the trigram model can provide more accurate predictions if the phrase 'red dead' is common in the dataset.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "While n-gram models are simple and provide a baseline for language modeling, they have limitations that can be addressed with more advanced models like neural language models, which capture longer dependencies and semantic meanings better."
      ],
      "metadata": {
        "id": "qsvpPZUJQklP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Extend the language model to perform basic **sentiment analysis**. Perform some descriptive analysis and then use the n-grams and Word frequency to predict the sentiment of the sentences in your text (positive, negative and, if you want, neutral) instead of just predicting the next Word. Don't forget to evaluate the performance of your model and comment on the results."
      ],
      "metadata": {
        "id": "34hxR6zmR6xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some descriptive statistics\n",
        "print(f\"Total number of unigrams: {len(unigram_freq)}\")\n",
        "print(f\"Total number of bigrams: {len(bigram_freq)}\")\n",
        "print(f\"Total number of trigrams: {len(trigram_freq)}\")\n",
        "print(f\"Most common unigrams: {unigram_freq.most_common(10)}\")\n",
        "print(f\"Most common bigrams: {bigram_freq.most_common(10)}\")\n",
        "print(f\"Most common trigrams: {trigram_freq.most_common(10)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97JDfUySdgF",
        "outputId": "e3e1d105-ea6c-46e5-8acd-cffa0cb953b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unigrams: 31115\n",
            "Total number of bigrams: 338250\n",
            "Total number of trigrams: 693150\n",
            "Most common unigrams: [('the', 44611), ('i', 36164), ('to', 29042), ('and', 26712), ('a', 24307), ('of', 19528), ('it', 17941), ('is', 17883), ('in', 15795), ('for', 15672)]\n",
            "Most common bigrams: [('i m', 4531), ('it s', 3716), ('twitter com', 3708), ('pic twitter', 3511), ('of the', 2862), ('in the', 2842), ('can t', 2377), ('don t', 2369), ('and i', 2197), ('t co', 2087)]\n",
            "Most common trigrams: [('pic twitter com', 3511), ('https t co', 2085), ('red dead redemption', 1077), ('i can t', 937), ('_ _ _', 936), ('call of duty', 835), ('italy italy italy', 761), ('i don t', 735), ('dead redemption 2', 708), ('league of legends', 651)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Sentiments**\n",
        "\n",
        "We have sentiment labels (positive, negative, neutral) for each tweet in the dataset so we will use these labels to train a sentiment classifier."
      ],
      "metadata": {
        "id": "ybmFX_JMUIdB"
      }
    }
  ]
}